{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucaokylong/speech_recognition_simple_word/blob/main/VoiceRec_VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zoEMU-q68sv"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q tensorflow tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ],
      "metadata": {
        "id": "SB3-aKVDDXGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit_keras"
      ],
      "metadata": {
        "id": "Diw7WtRRLWlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "r3Dz4Y86Lb0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "h7VdN9RDS0E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from IPython import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Flatten, Dropout, Activation, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from vit_keras import vit, utils\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5S9COD7Z7QBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import the mini Speech Commands dataset**"
      ],
      "metadata": {
        "id": "KmWwqQVj7ax_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = 'data/mini_speech_commands'\n",
        "\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ],
      "metadata": {
        "id": "x8bMLdZ17Qxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset's audio clips are stored in eight folders"
      ],
      "metadata": {
        "id": "C6mc6RY77wMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
        "print('Commands:', commands)"
      ],
      "metadata": {
        "id": "3ounZMfH7lXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divided into directories this way, you can easily load the data"
      ],
      "metadata": {
        "id": "WKUD1B2r8moC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=16000,\n",
        "    subset='both')\n",
        "\n",
        "label_names = np.array(train_ds.class_names)\n",
        "print()\n",
        "print(\"label names:\", label_names)"
      ],
      "metadata": {
        "id": "BkP-l3-o7yDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset now contains batches of audio clips and integer labels. The audio clips have a shape of (batch, samples, channels)."
      ],
      "metadata": {
        "id": "7XhiqeBl8xTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.element_spec"
      ],
      "metadata": {
        "id": "cm-ids9m8zXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset only contains single channel audio, so use the tf.squeeze function to drop the extra axis"
      ],
      "metadata": {
        "id": "TcGQ4fzN87GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def squeeze(audio, labels):\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  return audio, labels\n",
        "\n",
        "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "m7XbLKJ_88uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The utils.audio_dataset_from_directory function only returns up to two splits. It's a good idea to keep a test set separate from your validation set. Ideally you'd keep it in a separate directory, but in this case you can use Dataset.shard to split the validation set into two halves. Note that iterating over any shard will load all the data, and only keep its fraction."
      ],
      "metadata": {
        "id": "O3Q8JSle9bBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = val_ds.shard(num_shards=2, index=0)\n",
        "val_ds = val_ds.shard(num_shards=2, index=1)"
      ],
      "metadata": {
        "id": "wKjUpP6h9cKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot a few audio waveforms"
      ],
      "metadata": {
        "id": "y_zfZgWb9p2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for example_audio, example_labels in train_ds.take(1):  \n",
        "  print(example_audio.shape)\n",
        "  print(example_labels.shape)"
      ],
      "metadata": {
        "id": "gCnzjn5M9vIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names[[1,1,3,0]]"
      ],
      "metadata": {
        "id": "3KEQjaKF-Bdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows * cols\n",
        "for i in range(n):\n",
        "  plt.subplot(rows, cols, i+1)\n",
        "  audio_signal = example_audio[i]\n",
        "  plt.plot(audio_signal)\n",
        "  plt.title(label_names[example_labels[i]])\n",
        "  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  plt.ylim([-1.1, 1.1])"
      ],
      "metadata": {
        "id": "1aQ3Bfmp-C_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert waveforms to spectrograms\n",
        "Transform the waveforms from the time-domain signals into the time-frequency-domain signals by computing the short-time Fourier transform (STFT) to convert the waveforms to as spectrograms, which show frequency changes over time and can be represented as 2D images. You will feed the spectrogram images into your neural network to train the model."
      ],
      "metadata": {
        "id": "GS5T722l-LiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spectrogram(waveform):\n",
        "  # Convert the waveform to a spectrogram via a STFT.\n",
        "  spectrogram = tf.signal.stft(\n",
        "      waveform, frame_length=256, frame_step=128)\n",
        "  # Obtain the magnitude of the STFT.\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  # Add a `channels` dimension, so that the spectrogram can be used\n",
        "  # as image-like input data with convolution layers (which expect\n",
        "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  #spectrogram = spectrogram[..., tf.newaxis]\n",
        "  #spectrogram = np.stack((spectrogram,)*3, axis=-1)\n",
        "  spectrogram = tf.repeat(spectrogram[..., tf.newaxis], 3, -1)\n",
        "  return spectrogram"
      ],
      "metadata": {
        "id": "TDG2VEu5-bB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio"
      ],
      "metadata": {
        "id": "yfLC1Ccq-lpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  label = label_names[example_labels[i]]\n",
        "  waveform = example_audio[i]\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "  print('Label:', label)\n",
        "  print('Waveform shape:', waveform.shape)\n",
        "  print('Spectrogram shape:', spectrogram.shape)\n",
        "  print('Audio playback')\n",
        "  display.display(display.Audio(waveform, rate=16000))"
      ],
      "metadata": {
        "id": "vPw-UgMQ-m6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for displaying a spectrogram"
      ],
      "metadata": {
        "id": "yD_l1tNu-3yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "  # Convert the frequencies to log scale and transpose, so that the time is\n",
        "  # represented on the x-axis (columns).\n",
        "  # Add an epsilon to avoid taking a log of zero.\n",
        "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)"
      ],
      "metadata": {
        "id": "XatE_DiB-2JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the example's waveform over time and the corresponding spectrogram (frequencies over time)"
      ],
      "metadata": {
        "id": "8kMzP18B_rEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.suptitle(label.title())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yYDu7Act_kyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create spectrogramn datasets from the audio datasets"
      ],
      "metadata": {
        "id": "uTF545jCVoUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_spec_ds(ds):\n",
        "  return ds.map(\n",
        "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
        "      num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "NR8UH25wVp8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_spectrogram_ds = make_spec_ds(train_ds)\n",
        "val_spectrogram_ds = make_spec_ds(val_ds)\n",
        "test_spectrogram_ds = make_spec_ds(test_ds)"
      ],
      "metadata": {
        "id": "NG6b85rFVttd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine the spectrograms for different examples of the dataset"
      ],
      "metadata": {
        "id": "y4Kb0QQWV5w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
        "  break"
      ],
      "metadata": {
        "id": "7u-xHrslVyiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
        "\n",
        "for i in range(n):\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
        "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oSGK87YSV6x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and train the model"
      ],
      "metadata": {
        "id": "HtbWLFvXWJp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "ZqgkqxxwWLo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XXzyuyYi4olC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = example_spectrograms.shape[1:]\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(label_names)\n",
        "image_size = 32\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "#norm_layer = tensorflow.keras.layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "#norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "#base_model = vit.vit_b16(image_size=(32,32), pretrained=True,\n",
        "                            #include_top=False, pretrained_top=False)\n",
        "\n",
        "\n",
        "def build_model():\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # Downsample the input.\n",
        "    #Resizing(32, 32),\n",
        "    x = tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (image_size, image_size)))(inputs)\n",
        "    base_model = vit.vit_b16(image_size=image_size, pretrained=True,\n",
        "                            include_top=False, pretrained_top=False)\n",
        "    x = base_model(x)\n",
        "    # Normalize.\n",
        "    #norm_layer,\n",
        "    x = Flatten()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Dense(64, activation='gelu')(x)\n",
        "    x = Dense(64, activation='gelu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(num_labels, activation=\"softmax\")(x)\n",
        "\n",
        "    model_final = Model(inputs = inputs, outputs = outputs)\n",
        "    return model_final\n",
        "\n",
        "'''\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x) \n",
        "    '''"
      ],
      "metadata": {
        "id": "cVBH19m1WPjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Z-g8pg_4y-Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrd = ReduceLROnPlateau(monitor = 'val_accuracy',patience = 5,verbose = 1,factor = 0.5, min_lr = 0.00001)\n",
        "check_point = ModelCheckpoint('modelcp2.h5',\n",
        "                              save_best_only= True, mode = 'auto')\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "EPOCHS = 30\n",
        "history = model.fit(\n",
        "    train_spectrogram_ds,\n",
        "    validation_data=val_spectrogram_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks= [lrd, check_point]\n",
        ")\n",
        "model.save(\"voicerec2.h5\")"
      ],
      "metadata": {
        "id": "21o5gJFiWbS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the training and validation loss curves"
      ],
      "metadata": {
        "id": "DZioFEw4iHOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = history.history\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [CrossEntropy]')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.ylim([0, 100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy [%]')"
      ],
      "metadata": {
        "id": "vsJb5fV4YT5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}